# -*- coding: utf-8 -*-
"""MODEL_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WPsd8F3mA9qKI33zKmJmjApAzIG1yTnR
"""

from google.colab import drive
drive.mount('/content/drive')

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
# Calculate and print metrics
r2 = r2_score(y_val, y_val_pred)
mse = mean_squared_error(y_val, y_val_pred)
mae = mean_absolute_error(y_val, y_val_pred)

print(f"R-squared: {r2}")
print(f"Mean Squared Error: {mse}")
print(f"Mean Absolute Error: {mae}")

from IPython import get_ipython
from IPython.display import display
# %%
from google.colab import drive
drive.mount('/content/drive')
# %%
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import tensorflow as tf
from tensorflow import keras

# Load datasets
train_df = pd.read_csv("/content/drive/MyDrive/combfine.csv")  # Ensure the correct path
test_df = pd.read_csv("/content/drive/MyDrive/Copy of Body_measurements_of_507_physically_active_individuals.csv")

# Check for any missing values, and correct them
print("Missing values in train_df:")
print(train_df.isnull().sum())
print("Missing values in test_df:")
print(test_df.isnull().sum())
train_df.dropna(inplace=True)
test_df.dropna(inplace=True)

# Display train_df columns:
print("train_df columns:", train_df.columns)
# Display test_df columns:
print("test_df columns:", test_df.columns)

# check if any columns are being dropped by mistake, there are no mistakes
# there is only one column in the first dataset that is not in the second.
# Define input features and target
X = train_df.drop(columns=["Hips"])
y = train_df["Hips"]

# Split data into training and test sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
print("X_train columns:", X_train.columns)
print("test_df columns:", test_df.columns)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
# Scale the test data using the same scaler
X_test_scaled = scaler.transform(test_df)

# Build a simpler Deep Learning Model
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), epochs=50, batch_size=32, verbose=1)

# Predict Hips for the new dataset
hip_predictions = model.predict(X_test_scaled).flatten()
y_val_pred = model.predict(X_val_scaled).flatten()

# Add predictions to the test dataset
test_df["Predicted Hips"] = hip_predictions

# Save the output as an Excel file
test_df.to_excel("Predicted_Hips.xlsx", index=False)

# Calculate and print metrics
r2 = r2_score(y_val, y_val_pred)
mse = mean_squared_error(y_val, y_val_pred)
mae = mean_absolute_error(y_val, y_val_pred)
rmse = np.sqrt(mse)

print(f"R-squared: {r2}")
print(f"Mean Squared Error: {mse}")
print(f"Mean Absolute Error: {mae}")
print(f"Root Mean Squared Error: {rmse}")

print("Predictions saved to Predicted_Hips.xlsx!")

#main model training
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score
import joblib
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/Realdata.csv")
df.columns = df.columns.str.strip()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import zscore
# Plot before outlier removal
plt.figure(figsize=(10, 5))
sns.boxplot(data=df[['Height', 'Weight', 'Chest', 'Shoulder', 'Waist', 'Hip']])
plt.title("Before Outlier Removal")
plt.show()

# Remove outliers using Z-score
df = df[(np.abs(zscore(df[['Height', 'Weight', 'Chest', 'Shoulder', 'Waist', 'Hip']])) < 3).all(axis=1)]

# Plot after outlier removal
plt.figure(figsize=(10, 5))
sns.boxplot(data=df[['Height', 'Weight', 'Chest', 'Shoulder', 'Waist', 'Hip']])
plt.title("After Outlier Removal")
plt.show()

# Split dataset by gender (0 = Female, 1 = Male)
df_male = df[df['Gender'] == 1]
df_female = df[df['Gender'] == 0]

def train_xgboost_model(data, gender):
    """Train and tune XGBoost model to predict Chest, Shoulder, Waist, and Hip based on Height & Weight"""
    X = data[['Height', 'Weight']]
    y = data[['Chest', 'Shoulder', 'Waist', 'Hip']]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    # Define XGBoost model
    model = xgb.XGBRegressor()

    # Hyperparameter tuning
    param_grid = {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 5, 7],
        'min_child_weight': [1, 3, 5]
    }
    grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_absolute_error', cv=3, verbose=1)
    grid_search.fit(X_train, y_train)

    # Best model
    best_model = grid_search.best_estimator_

    # Predict and evaluate
    y_pred = best_model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Compute accuracy by comparing actual vs predicted within a threshold
    threshold = 3  # cm
    accuracy = np.mean(np.abs(y_test - y_pred) <= threshold) * 100

    print(f"{gender} Model Evaluation:")
    print(f"Mean Absolute Error (MAE): {mae:.2f} cm")
    print(f"Mean Squared Error (MSE): {mse:.2f}")
    print(f"R-Squared Score (R2): {r2:.4f}")
    print(f"Model Accuracy (within ±{threshold} cm): {accuracy:.2f}%")

    # Save model
    # joblib.dump(best_model, f"/content/drive/MyDrive/{gender}_xgboost_model.pkl")
    # print(f"{gender} model saved successfully!")

    return best_model

# Train and save models for male and female
male_model = train_xgboost_model(df_male, "Male")
female_model = train_xgboost_model(df_female, "Female")

print("XGBoost models trained and saved successfully!")

!pip install optuna

import pandas as pd
import xgboost as xgb
import numpy as np
import optuna
import joblib
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.stats import zscore
from sklearn.ensemble import RandomForestRegressor

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/Realdata.csv")
df.columns = df.columns.str.strip()
# Feature Engineering: Add BMI
df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)

# Remove Outliers
cols = ['Height', 'Weight', 'Chest', 'Shoulder', 'Waist', 'Hip']
df = df[(np.abs(zscore(df[cols])) < 3).all(axis=1)]

# Split dataset by gender (0 = Female, 1 = Male)
df_male = df[df['Gender'] == 1]
df_female = df[df['Gender'] == 0]

def train_xgboost_model(data, gender):
    """Train and tune XGBoost model with improved feature engineering and hyperparameter tuning"""
    X = data[['Height', 'Weight', 'BMI']]
    y = data[['Chest', 'Shoulder', 'Waist', 'Hip']]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Standard Scaling
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    def objective(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 500),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10)
        }
        model = xgb.XGBRegressor(**params)
        model.fit(X_train, y_train)
        return mean_absolute_error(y_test, model.predict(X_test))

    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=50)
    best_params = study.best_params
    best_model = xgb.XGBRegressor(**best_params)
    best_model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred = best_model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Compute accuracy by comparing actual vs predicted within a threshold
    threshold = 3  # cm
    accuracy = np.mean(np.abs(y_test - y_pred) <= threshold) * 100

    print(f"{gender} Model Evaluation:")
    print(f"Mean Absolute Error (MAE): {mae:.2f} cm")
    print(f"Mean Squared Error (MSE): {mse:.2f}")
    print(f"R-Squared Score (R2): {r2:.4f}")
    print(f"Model Accuracy (within ±{threshold} cm): {accuracy:.2f}%")

    # Save model
    # joblib.dump(best_model, f"/content/drive/MyDrive/{gender}_xgboost_model.pkl")
    # print(f"{gender} model saved successfully!")

    return best_model

# Train and save models for male and female
male_model = train_xgboost_model(df_male, "Male")
female_model = train_xgboost_model(df_female, "Female")

print("XGBoost models trained and saved successfully!")

#Random forest model
import pandas as pd
import numpy as np
import joblib
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.stats import zscore
from sklearn.ensemble import RandomForestRegressor

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/Realdata.csv")
df.columns = df.columns.str.strip()
# Feature Engineering: Add BMI
df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)

# Remove Outliers
cols = ['Height', 'Weight', 'Chest', 'Shoulder', 'Waist', 'Hip']
df = df[(np.abs(zscore(df[cols])) < 3).all(axis=1)]

# Split dataset by gender (0 = Female, 1 = Male)
df_male = df[df['Gender'] == 1]
df_female = df[df['Gender'] == 0]

def train_random_forest_model(data, gender):
    """Train and tune Random Forest model with feature engineering and hyperparameter tuning"""
    X = data[['Height', 'Weight', 'BMI']]
    y = data[['Chest', 'Shoulder', 'Waist', 'Hip']]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Standard Scaling
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Define Random Forest model with hyperparameter tuning
    best_model = RandomForestRegressor(n_estimators=300, max_depth=10, min_samples_split=2, min_samples_leaf=1, random_state=42)
    best_model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred = best_model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Compute accuracy by comparing actual vs predicted within a threshold
    threshold = 3  # cm
    accuracy = np.mean(np.abs(y_test - y_pred) <= threshold) * 100

    print(f"{gender} Model Evaluation:")
    print(f"Mean Absolute Error (MAE): {mae:.2f} cm")
    print(f"Mean Squared Error (MSE): {mse:.2f}")
    print(f"R-Squared Score (R2): {r2:.4f}")
    print(f"Model Accuracy (within ±{threshold} cm): {accuracy:.2f}%")

    # Save model
    # joblib.dump(best_model, f"/content/drive/MyDrive/{gender}_random_forest_model.pkl")
    # print(f"{gender} model saved successfully!")

    return best_model

# Train and save models for male and female
male_model = train_random_forest_model(df_male, "Male")
female_model = train_random_forest_model(df_female, "Female")

print("Random Forest models trained and saved successfully!")

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import zscore

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/Realdata.csv")
df.columns = df.columns.str.strip()
# Feature Engineering: Add BMI
df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)

# Remove Outliers
cols = ['Height', 'Weight', 'Chest', 'Shoulder', 'Waist', 'Hip']
df = df[(np.abs(zscore(df[cols])) < 3).all(axis=1)]

# Split dataset by gender (0 = Female, 1 = Male)
df_male = df[df['Gender'] == 1]
df_female = df[df['Gender'] == 0]

def train_mlp_model(data, gender):
    """Train and tune an MLP deep learning model."""
    X = data[['Height', 'Weight', 'BMI']]
    y = data[['Chest', 'Shoulder', 'Waist', 'Hip']]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Standard Scaling
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Define MLP model with hyperparameter tuning
    model = Sequential([
        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        BatchNormalization(),
        Dropout(0.3),
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(4, activation='linear')  # Output layer for regression
    ])

    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])

    # Train model
    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=32, verbose=1)

    # Predict and evaluate
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Compute accuracy by comparing actual vs predicted within a threshold
    threshold = 3  # cm
    accuracy = np.mean(np.abs(y_test - y_pred) <= threshold) * 100

    print(f"{gender} Model Evaluation:")
    print(f"Mean Absolute Error (MAE): {mae:.2f} cm")
    print(f"Mean Squared Error (MSE): {mse:.2f}")
    print(f"R-Squared Score (R2): {r2:.4f}")
    print(f"Model Accuracy (within ±{threshold} cm): {accuracy:.2f}%")

    # Save model
    model.save(f"/content/drive/MyDrive/{gender}_mlp_model.h5")
    print(f"{gender} model saved successfully!")

    return model

# Train and save models for male and female
male_model = train_mlp_model(df_male, "Male")
female_model = train_mlp_model(df_female, "Female")

print("MLP deep learning models trained and saved successfully!")

# RNN

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import zscore

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/Realdata.csv")
df.columns = df.columns.str.strip()
# Feature Engineering: Add BMI
df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)

# Remove Outliers
cols = ['Height', 'Weight', 'Chest', 'Shoulder', 'Waist', 'Hip']
df = df[(np.abs(zscore(df[cols])) < 3).all(axis=1)]

# Normalize Target Variables
target_scaler = MinMaxScaler()
df[['Chest', 'Shoulder', 'Waist', 'Hip']] = target_scaler.fit_transform(df[['Chest', 'Shoulder', 'Waist', 'Hip']])

# Split dataset by gender (0 = Female, 1 = Male)
df_male = df[df['Gender'] == 1]
df_female = df[df['Gender'] == 0]

def train_rnn_model(data, gender):
    """Train and tune an RNN model."""
    X = data[['Height', 'Weight', 'BMI']]
    y = data[['Chest', 'Shoulder', 'Waist', 'Hip']]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Standard Scaling
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Reshape input data for RNN
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

    # Define RNN model
    model = Sequential([
        SimpleRNN(128, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], 1)),
        BatchNormalization(),
        Dropout(0.3),
        SimpleRNN(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(4, activation='linear')  # Output layer for regression
    ])

    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])

    # Train model
    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=32, verbose=1)

    # Predict and evaluate
    y_pred = model.predict(X_test)
    y_pred = target_scaler.inverse_transform(y_pred)
    y_test = target_scaler.inverse_transform(y_test)

    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Compute accuracy by comparing actual vs predicted within a threshold
    threshold = 3  # cm
    accuracy = np.mean(np.abs(y_test - y_pred) <= threshold) * 100

    print(f"{gender} Model Evaluation:")
    print(f"Mean Absolute Error (MAE): {mae:.2f} cm")
    print(f"Mean Squared Error (MSE): {mse:.2f}")
    print(f"R-Squared Score (R2): {r2:.4f}")
    print(f"Model Accuracy (within ±{threshold} cm): {accuracy:.2f}%")

    # Save model
    model.save(f"/content/drive/MyDrive/{gender}_rnn_model.h5")
    print(f"{gender} model saved successfully!")

    return model

# Train and save models for male and female
male_model = train_rnn_model(df_male, "Male")
female_model = train_rnn_model(df_female, "Female")

print("RNN deep learning models trained and saved successfully!")